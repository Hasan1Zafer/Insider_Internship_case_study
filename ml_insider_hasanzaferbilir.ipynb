{
 "cells": [
  {
   "cell_type": "raw",
   "id": "af93ec8a-a07c-49c7-9427-bd69ba5cc0c0",
   "metadata": {},
   "source": [
    "# Multiclass Classification Model for Product Descriptions\n",
    "\n",
    "**Author:** Hasan Zafer Bilir\n",
    "\n",
    "This notebook documents the steps taken to build, train, deploy, and test a multiclass classification model for predicting product categories based on their descriptions. The project includes data exploration, preprocessing, model training, deployment using Docker, and creating an inference interface.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8d1b7d0-9a37-4b51-85c5-2eb9d5f779b0",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "## Data Exploration\n",
    "\n",
    "In this section, we will load the dataset and perform an initial exploration to understand its structure and characteristics.\n",
    "The data preprocessing steps are crucial for ensuring that our machine learning model receives clean and well-structured data. In this section, we will cover the following steps:\n",
    "\n",
    "1. Loading the data with error handling\n",
    "2. Text preprocessing\n",
    "3. Merging datasets and handling missing values\n",
    "4. Saving the processed data\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "145475ab-0123-4388-8ba1-9ac580f66795",
   "metadata": {},
   "source": [
    "Loading the Data\n",
    "\n",
    "Description: A custom load_data function with error handling is used to load product categories and descriptions from text files. This property counts the number of columns in each line, divides it into two pieces using a semicolon (;), and then determines if the two parts match. If not, the content is skipped and a notice is printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11285ab9-1c17-4d67-8a51-e7f938ca4669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Data loading function with error handling\n",
    "def load_data(file_path, columns):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            try:\n",
    "                parts = line.strip().split(';')\n",
    "                if len(parts) == len(columns):\n",
    "                    data.append(parts)\n",
    "                else:\n",
    "                    print(f\"Skipping line {line_number}: {line.strip()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line {line_number}: {e}\")\n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Load data\n",
    "product_categories = load_data('data/Product_Categories.txt', ['product_id', 'category']) # Load data\n",
    "\n",
    "product_explanation = load_data('data/Product_Explanation.txt', ['product_id', 'description']) # Load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58b1823e-606a-4520-9c85-6e3a4adbd0c7",
   "metadata": {},
   "source": [
    "Text preprocessing\n",
    "\n",
    "The basic function Iâ€™m gonna define is a text pre-processing function called preprocess_text that converts text to lowercase and removes punctuation. With this, we can ensure that the text is in a consistent format for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146f4665-d3a5-41dd-9d23-47c0412b57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# basic function for text preprocessing\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04d5c3d9-c8b0-4073-a889-be20763f0811",
   "metadata": {},
   "source": [
    "Merging datasets and handling missing values\n",
    "\n",
    "I join the product categories and explanations datasets on the product_id column. I next clear all null rows and use a text preprocessing function on the description column, with the findings saved in a new column called processed_description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde6adad-db2a-48d1-b005-83240758a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/processed_data.csv', index=False)\n",
    "print(\"Data preprocessing completed and saved to 'data/processed_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07af01a9-eab4-43d5-9b7a-303226235713",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "In this section, we will train multiple machine learning models and evaluate their performance. We will use GridSearchCV for hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0dd9f6a8-c877-4fba-990a-e9cfc8545fa8",
   "metadata": {},
   "source": [
    "Step 1: Loading and Handling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6659db4-aaee-4678-8657-374e078719ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Load the preprocessed data\n",
    "data = pd.read_csv('data/processed_data.csv')\n",
    "\n",
    "# Handle missing values\n",
    "data['processed_description'] = data['processed_description'].fillna('')\n",
    "data['category'] = data['category'].fillna('unknown')  # Replace NaNs in category with a placeholder\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec12697d-6552-4d8c-98d3-678e9732a8d1",
   "metadata": {},
   "source": [
    "Step 2: Removing Low-Frequency Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0203c62-a5a2-40dc-b6ec-9b495694c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove categories with fewer than 2 samples\n",
    "category_counts = data['category'].value_counts()\n",
    "data = data[data['category'].isin(category_counts[category_counts > 1].index)]\n",
    "\n",
    "# Further remove categories with very few samples (e.g., fewer than 5 samples)\n",
    "min_samples = 5\n",
    "data = data[data['category'].isin(category_counts[category_counts >= min_samples].index)]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e5d0ebf-d5d0-43df-b192-2c7558c804ff",
   "metadata": {},
   "source": [
    "Step 3: Ensuring Sufficient Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b49ac70-e61c-4183-9269-654fd6be4287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of classes\n",
    "num_classes = data['category'].nunique()\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Ensure the training set size is at least equal to the number of classes\n",
    "min_train_size = num_classes\n",
    "min_data_points = 2 * num_classes\n",
    "if len(data) < min_data_points:\n",
    "    raise ValueError(f\"Insufficient data: The dataset must have at least {min_data_points} samples to ensure proper splitting.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51dc675f-6fd7-4e1e-b7f3-a17580b61220",
   "metadata": {},
   "source": [
    "Step 4: Vectorizing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581118b-0aee-4f2e-a406-af9eb9f388ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data['processed_description'])\n",
    "y = data['category']\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52e626a5-856a-4c67-95cc-82ee2d283dd4",
   "metadata": {},
   "source": [
    "Step 5: Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f5afb8-846c-4b02-a3a9-d31809d267c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "test_size = 1 - (min_train_size / len(data))  # Adjust test size to ensure train size is sufficient\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)\n",
    "\n",
    "# Check the distribution of categories\n",
    "print(\"Training set category distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTesting set category distribution:\")\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6b6cb9a-a223-445a-a8ff-184c2ca46865",
   "metadata": {},
   "source": [
    "Step 6: Defining Models and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85213c5c-6ac0-4f1b-91a6-914f9180373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Define models and hyperparameters\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'GradientBoosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20]\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ceb97766-c52c-4f19-beaa-4b8a03b9cdc8",
   "metadata": {},
   "source": [
    "Step 7: Performing GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49ad3c-3741-4c98-b072-1359e99ac336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Perform GridSearchCV\n",
    "best_models = {}\n",
    "# Dynamically set n_splits based on the minimum number of samples in any class\n",
    "n_splits = max(2, min(5, min(y_train.value_counts())))  # Ensure n_splits is at least 2\n",
    "stratified_k_fold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    grid_search = GridSearchCV(models[model_name], param_grids[model_name], cv=stratified_k_fold, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    y_pred = best_models[model_name].predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4d453fc-da07-4a55-9b8e-ec40e2b1d85a",
   "metadata": {},
   "source": [
    "Step 8: Saving the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65df466-4f32-426e-a317-b29cd69c3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and the vectorizer\n",
    "best_model = best_models['GradientBoosting']  # Choose the best model based on performance\n",
    "joblib.dump(best_model, 'models/model.pkl')\n",
    "joblib.dump(vectorizer, 'models/vectorizer.pkl')\n",
    "\n",
    "print(\"Model training completed and saved to 'models/' directory.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf579243-d91d-4d7a-aa7b-b6e13f128b22",
   "metadata": {},
   "source": [
    "## Deployment and Inference Interface\n",
    "\n",
    "Here is the content of inference.py:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb6a58-e33d-4ede-b222-6c871158481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load the model \n",
    "model = joblib.load('models/model.pkl')\n",
    "vectorizer = joblib.load('models/vectorizer.pkl')\n",
    "\n",
    "#imitialize\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Get the product description from the request\n",
    "    data = request.get_json(force=True)\n",
    "    description = data['description']\n",
    "    \n",
    "    # Preprocess the description and make a prediction\n",
    "    processed_description = vectorizer.transform([description])\n",
    "    prediction = model.predict(processed_description)\n",
    "    \n",
    "    # Create a response with the prediction\n",
    "    response = {\n",
    "        'description': description,\n",
    "        'category': prediction[0]\n",
    "    }\n",
    "    return jsonify(response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, host='0.0.0.0', port=5000)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bc2d983-0169-4644-9953-e0feaf66070c",
   "metadata": {},
   "source": [
    "DOCKERFÄ°LE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31007298-582c-4c13-a44b-1c8b1ec6b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an official Python runtime as a parent image\n",
    "FROM python:3.8-slim\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "COPY . /app\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Make port 5000 available to the world outside this container\n",
    "EXPOSE 5000\n",
    "\n",
    "# Define environment variable\n",
    "ENV NAME World\n",
    "\n",
    "# Run inference.py when the container launches\n",
    "CMD [\"python\", \"inference.py\"]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32c7524c-e961-490b-98af-c7756796f626",
   "metadata": {},
   "source": [
    "\n",
    "SUMMARY AND CONCLUSÄ°ON\n",
    "\n",
    "ï»¿Project Report: Multiclass Classification Model for Product Descriptions\n",
    "Summary of Approach, Findings, and Insights\n",
    "1. Approach\n",
    "\n",
    "    Data Preprocessing:\n",
    "        Loaded product categories and outlines from textual content files.\n",
    "        Merged the datasets on product_id.\n",
    "        Handled missing values by way of filling them with suitable placeholders.\n",
    "        Preprocessed textual content records through converting to lowercase and eliminating punctuation.\n",
    "        Removed classes with fewer than 2 samples to make sure enough information for every category.\n",
    "        Further filtered out classes with fewer than 5 samples to enhance version schooling robustness.\n",
    "\n",
    "    Model Training:\n",
    "        Vectorized the text statistics the usage of TF-IDF.\n",
    "        Split the information into schooling and testing sets the usage of stratified sampling to maintain category distribution.\n",
    "        Defined a couple of gadget mastering models (Logistic Regression, Random Forest, Gradient Boosting) with hyperparameters.\n",
    "        Used GridSearchCV with stratified ok-fold pass-validation to find the first-rate hyperparameters for every model.\n",
    "        Evaluated the overall performance of every version and decided on the fine one based totally on accuracy and category file.\n",
    "\n",
    "    Deployment:\n",
    "        Created a Flask application to serve the educated model.\n",
    "        Containerized the application using Docker to make sure it can be effortlessly deployed in any surroundings.\n",
    "        Provided a easy, person-friendly inference interface to make predictions based on product descriptions.\n",
    "\n",
    "2. Findings and Insights\n",
    "\n",
    "    Model Selection:\n",
    "        The Gradient Boosting model with GridSearchCV achieved the best prediction accuracy of 0.26.\n",
    "        Logistic Regression and Random Forest fashions finished worse, with prediction accuracies of 0.12 and 0.20, respectively.\n",
    "        The use of GridSearchCV substantially improved version overall performance with the aid of finding the most efficient hyperparameters.\n",
    "\n",
    "    Data Distribution:\n",
    "        The dataset had a massive range of categories with only a few samples, which made it hard to educate robust models.\n",
    "        Removing low-frequency classes helped in stabilizing the model education procedure.\n",
    "\n",
    "    Text Preprocessing:\n",
    "        Preprocessing text by lowercasing and casting off punctuation changed into powerful in standardizing the enter records.\n",
    "\n",
    "Analysis of Model Performance and Discussion of Results\n",
    "1. Model Performance\n",
    "\n",
    "    Gradient Boosting:\n",
    "        Achieved the highest prediction accuracy of zero.26.\n",
    "        Performed nicely in managing the imbalance within the dataset and capturing the relationships in the text statistics.\n",
    "\n",
    "    Logistic Regression:\n",
    "        Achieved a prediction accuracy of 0.12.\n",
    "        Struggled with the complexity of the multiclass type problem and the large wide variety of categories.\n",
    "\n",
    "    Random Forest:\n",
    "        Achieved a prediction accuracy of 0.20.\n",
    "        Performed better than Logistic Regression but turned into still outperformed by means of Gradient Boosting.\n",
    "\n",
    "2. Discussion of Results\n",
    "\n",
    "    GridSearchCV:\n",
    "        We selected GridSearchCV for hyperparameter tuning as it systematically explores one of a kind hyperparameter combos to find the pleasant model configuration.\n",
    "        This method is especially useful for enhancing model performance and finding the premiere stability among bias and variance.\n",
    "\n",
    "    Challenges:\n",
    "        The large quantity of classes with few samples posed a sizable project. This often ends in overfitting, where the version plays nicely at the training records but poorly on the checking out records.\n",
    "        Ensuring a sufficient amount of facts for each class is essential for constructing robust models.\n",
    "\n",
    "Errors Faced and Solutions\n",
    "\n",
    "    Error: Missing Values in Data:\n",
    "        Solution: Handled lacking values with the aid of filling them with suitable placeholders ('' for descriptions and 'unknown' for categories).\n",
    "\n",
    "    Error: Categories with Very Few Samples:\n",
    "        Solution: Removed categories with fewer than 2 samples and similarly filtered out categories with fewer than five samples to make sure enough training records.\n",
    "\n",
    "    Error: Imbalanced Data Distribution:\n",
    "        Solution: Used stratified sampling to preserve class distribution in training and trying out units, ensuring that every set had a consultant distribution of categories.\n",
    "\n",
    "    Error: Model Overfitting:\n",
    "        Solution: Employed GridSearchCV with cross-validation to locate the most reliable hyperparameters and reduce overfitting through balancing model complexity.\n",
    "\n",
    "Additional Observations and Recommendations\n",
    "\n",
    "    Data Augmentation:\n",
    "        Consider facts augmentation strategies to artificially growth the scale of the dataset and improve model robustness.\n",
    "        Techniques together with synonym replacement, returned-translation, and paraphrasing can be used to generate greater education samples.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
